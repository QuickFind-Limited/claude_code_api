"""
Comprehensive Retail SKU Analysis Framework
==========================================

A practical framework for analyzing top-performing SKUs and identifying sales opportunities
in retail operations. This framework includes data collection, performance metrics, analysis
methodologies, and implementation strategies.

Author: Data Analytics Team
Version: 1.0
Date: 2025-01-13
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class RetailSKUAnalyzer:
    """
    Comprehensive SKU analysis framework for retail operations
    """
    
    def __init__(self, config: Dict = None):
        """Initialize the analyzer with configuration parameters"""
        self.config = config or self._get_default_config()
        self.data = {}
        self.metrics = {}
        self.opportunities = []
        
    def _get_default_config(self) -> Dict:
        """Default configuration parameters"""
        return {
            'analysis_period_days': 365,
            'top_performers_threshold': 0.8,  # Top 80% of sales
            'seasonality_periods': [30, 90, 365],  # Days for seasonality analysis
            'inventory_turnover_threshold': 4,  # Minimum acceptable turnover
            'price_elasticity_threshold': 0.1,
            'opportunity_score_weights': {
                'revenue_impact': 0.3,
                'margin_impact': 0.25,
                'feasibility': 0.25,
                'risk': 0.2
            }
        }

class DataCollectionFramework:
    """
    Data collection requirements and validation framework
    """
    
    @staticmethod
    def get_required_data_schema() -> Dict[str, Dict]:
        """
        Returns the required data schema for comprehensive SKU analysis
        """
        return {
            'sales_data': {
                'required_columns': [
                    'sku_id', 'transaction_date', 'quantity_sold', 'unit_price',
                    'total_revenue', 'discount_amount', 'store_id', 'channel'
                ],
                'optional_columns': [
                    'customer_id', 'promotion_code', 'sales_rep_id', 'margin',
                    'cost_of_goods', 'tax_amount'
                ],
                'data_types': {
                    'sku_id': 'string',
                    'transaction_date': 'datetime',
                    'quantity_sold': 'int',
                    'unit_price': 'float',
                    'total_revenue': 'float'
                }
            },
            
            'inventory_data': {
                'required_columns': [
                    'sku_id', 'date', 'on_hand_quantity', 'available_quantity',
                    'committed_quantity', 'unit_cost'
                ],
                'optional_columns': [
                    'reorder_point', 'reorder_quantity', 'lead_time_days',
                    'supplier_id', 'storage_cost', 'holding_cost'
                ],
                'data_types': {
                    'sku_id': 'string',
                    'date': 'datetime',
                    'on_hand_quantity': 'int',
                    'unit_cost': 'float'
                }
            },
            
            'customer_data': {
                'required_columns': [
                    'customer_id', 'customer_segment', 'acquisition_date'
                ],
                'optional_columns': [
                    'lifetime_value', 'preferred_channel', 'geographic_region',
                    'demographic_age_group', 'purchase_frequency_score'
                ],
                'data_types': {
                    'customer_id': 'string',
                    'acquisition_date': 'datetime'
                }
            },
            
            'pricing_data': {
                'required_columns': [
                    'sku_id', 'effective_date', 'list_price', 'cost_price'
                ],
                'optional_columns': [
                    'competitor_price', 'promotion_price', 'minimum_price',
                    'maximum_price', 'price_tier'
                ],
                'data_types': {
                    'sku_id': 'string',
                    'effective_date': 'datetime',
                    'list_price': 'float',
                    'cost_price': 'float'
                }
            },
            
            'product_master': {
                'required_columns': [
                    'sku_id', 'product_name', 'category', 'subcategory',
                    'brand', 'launch_date'
                ],
                'optional_columns': [
                    'product_line', 'supplier_id', 'weight', 'dimensions',
                    'color', 'size', 'material', 'seasonal_indicator'
                ],
                'data_types': {
                    'sku_id': 'string',
                    'launch_date': 'datetime'
                }
            }
        }
    
    @staticmethod
    def validate_data_quality(data: pd.DataFrame, data_type: str) -> Dict[str, any]:
        """
        Validate data quality for a given dataset
        
        Args:
            data: DataFrame to validate
            data_type: Type of data ('sales_data', 'inventory_data', etc.)
        
        Returns:
            Dictionary with validation results
        """
        schema = DataCollectionFramework.get_required_data_schema()[data_type]
        results = {
            'is_valid': True,
            'missing_columns': [],
            'data_quality_issues': [],
            'recommendations': []
        }
        
        # Check required columns
        missing_cols = [col for col in schema['required_columns'] if col not in data.columns]
        if missing_cols:
            results['is_valid'] = False
            results['missing_columns'] = missing_cols
            results['recommendations'].append(f"Add missing columns: {missing_cols}")
        
        # Check data quality
        if not data.empty:
            # Check for duplicates
            duplicates = data.duplicated().sum()
            if duplicates > 0:
                results['data_quality_issues'].append(f"{duplicates} duplicate rows found")
                results['recommendations'].append("Remove duplicate rows")
            
            # Check for missing values in required columns
            for col in schema['required_columns']:
                if col in data.columns:
                    missing_pct = (data[col].isnull().sum() / len(data)) * 100
                    if missing_pct > 5:  # More than 5% missing
                        results['data_quality_issues'].append(
                            f"Column '{col}' has {missing_pct:.1f}% missing values"
                        )
                        results['recommendations'].append(
                            f"Address missing values in '{col}'"
                        )
        
        return results

class PerformanceMetricsCalculator:
    """
    Performance metrics and KPIs calculation framework
    """
    
    @staticmethod
    def calculate_core_metrics(sales_data: pd.DataFrame, inventory_data: pd.DataFrame = None) -> pd.DataFrame:
        """
        Calculate core performance metrics for SKUs
        
        Args:
            sales_data: Sales transaction data
            inventory_data: Inventory data (optional)
        
        Returns:
            DataFrame with calculated metrics per SKU
        """
        # Group by SKU for aggregations
        sku_metrics = sales_data.groupby('sku_id').agg({
            'total_revenue': ['sum', 'mean', 'count'],
            'quantity_sold': ['sum', 'mean'],
            'unit_price': ['mean', 'std'],
            'transaction_date': ['min', 'max']
        }).reset_index()
        
        # Flatten column names
        sku_metrics.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in sku_metrics.columns.values]
        
        # Rename columns for clarity
        column_mapping = {
            'total_revenue_sum': 'total_revenue',
            'total_revenue_mean': 'avg_transaction_value',
            'total_revenue_count': 'transaction_count',
            'quantity_sold_sum': 'total_quantity_sold',
            'quantity_sold_mean': 'avg_quantity_per_transaction',
            'unit_price_mean': 'avg_unit_price',
            'unit_price_std': 'price_volatility',
            'transaction_date_min': 'first_sale_date',
            'transaction_date_max': 'last_sale_date'
        }
        sku_metrics = sku_metrics.rename(columns=column_mapping)
        
        # Calculate additional metrics
        sku_metrics['days_selling'] = (
            sku_metrics['last_sale_date'] - sku_metrics['first_sale_date']
        ).dt.days + 1
        
        sku_metrics['daily_revenue'] = sku_metrics['total_revenue'] / sku_metrics['days_selling']
        sku_metrics['daily_quantity'] = sku_metrics['total_quantity_sold'] / sku_metrics['days_selling']
        
        # Revenue concentration (what % of total revenue this SKU represents)
        total_revenue = sku_metrics['total_revenue'].sum()
        sku_metrics['revenue_share_pct'] = (sku_metrics['total_revenue'] / total_revenue) * 100
        
        # Calculate cumulative revenue share for ABC analysis
        sku_metrics = sku_metrics.sort_values('total_revenue', ascending=False)
        sku_metrics['cumulative_revenue_pct'] = sku_metrics['revenue_share_pct'].cumsum()
        
        # ABC Classification
        def classify_abc(cumulative_pct):
            if cumulative_pct <= 80:
                return 'A'
            elif cumulative_pct <= 95:
                return 'B'
            else:
                return 'C'
        
        sku_metrics['abc_category'] = sku_metrics['cumulative_revenue_pct'].apply(classify_abc)
        
        # Add inventory metrics if available
        if inventory_data is not None:
            inventory_metrics = PerformanceMetricsCalculator._calculate_inventory_metrics(
                sales_data, inventory_data
            )
            sku_metrics = sku_metrics.merge(inventory_metrics, on='sku_id', how='left')
        
        return sku_metrics
    
    @staticmethod
    def _calculate_inventory_metrics(sales_data: pd.DataFrame, inventory_data: pd.DataFrame) -> pd.DataFrame:
        """Calculate inventory-related metrics"""
        # Average inventory levels
        avg_inventory = inventory_data.groupby('sku_id')['on_hand_quantity'].mean().reset_index()
        avg_inventory.columns = ['sku_id', 'avg_inventory_level']
        
        # Calculate inventory turnover (COGS / Average Inventory)
        # Using total quantity sold as proxy for COGS in units
        total_sold = sales_data.groupby('sku_id')['quantity_sold'].sum().reset_index()
        
        inventory_metrics = avg_inventory.merge(total_sold, on='sku_id', how='left')
        inventory_metrics['inventory_turnover'] = (
            inventory_metrics['quantity_sold'] / inventory_metrics['avg_inventory_level']
        ).fillna(0)
        
        # Days of inventory on hand
        inventory_metrics['days_of_inventory'] = (
            365 / inventory_metrics['inventory_turnover']
        ).replace([np.inf, -np.inf], 365).fillna(365)
        
        return inventory_metrics[['sku_id', 'avg_inventory_level', 'inventory_turnover', 'days_of_inventory']]
    
    @staticmethod
    def calculate_advanced_metrics(sales_data: pd.DataFrame, window_days: int = 30) -> pd.DataFrame:
        """
        Calculate advanced performance metrics
        
        Args:
            sales_data: Sales data with datetime index
            window_days: Rolling window for trend calculations
        
        Returns:
            DataFrame with advanced metrics
        """
        # Ensure data is sorted by date
        sales_data = sales_data.sort_values(['sku_id', 'transaction_date'])
        
        advanced_metrics = []
        
        for sku_id in sales_data['sku_id'].unique():
            sku_data = sales_data[sales_data['sku_id'] == sku_id].copy()
            
            # Daily aggregation
            daily_sales = sku_data.groupby('transaction_date').agg({
                'total_revenue': 'sum',
                'quantity_sold': 'sum'
            }).reset_index()
            
            # Set date as index for time series operations
            daily_sales.set_index('transaction_date', inplace=True)
            
            # Reindex to include all dates (fill missing with 0)
            date_range = pd.date_range(
                start=daily_sales.index.min(),
                end=daily_sales.index.max(),
                freq='D'
            )
            daily_sales = daily_sales.reindex(date_range, fill_value=0)
            
            # Calculate rolling metrics
            rolling_revenue = daily_sales['total_revenue'].rolling(window=window_days, min_periods=1)
            rolling_quantity = daily_sales['quantity_sold'].rolling(window=window_days, min_periods=1)
            
            # Trend analysis (slope of linear regression)
            revenue_trend = PerformanceMetricsCalculator._calculate_trend(
                daily_sales['total_revenue'].values
            )
            quantity_trend = PerformanceMetricsCalculator._calculate_trend(
                daily_sales['quantity_sold'].values
            )
            
            # Volatility (coefficient of variation)
            revenue_cv = daily_sales['total_revenue'].std() / daily_sales['total_revenue'].mean() if daily_sales['total_revenue'].mean() > 0 else 0
            quantity_cv = daily_sales['quantity_sold'].std() / daily_sales['quantity_sold'].mean() if daily_sales['quantity_sold'].mean() > 0 else 0
            
            # Seasonality detection (simple approach using autocorrelation)
            seasonality_score = PerformanceMetricsCalculator._detect_seasonality(
                daily_sales['total_revenue'].values
            )
            
            advanced_metrics.append({
                'sku_id': sku_id,
                'revenue_trend': revenue_trend,
                'quantity_trend': quantity_trend,
                'revenue_volatility': revenue_cv,
                'quantity_volatility': quantity_cv,
                'seasonality_score': seasonality_score,
                'growth_rate_30d': rolling_revenue.pct_change(periods=30).iloc[-1] if len(daily_sales) >= 30 else 0
            })
        
        return pd.DataFrame(advanced_metrics)
    
    @staticmethod
    def _calculate_trend(series: np.ndarray) -> float:
        """Calculate trend using simple linear regression slope"""
        if len(series) < 2:
            return 0
        
        x = np.arange(len(series))
        trend = np.polyfit(x, series, 1)[0]  # Slope of linear fit
        return trend
    
    @staticmethod
    def _detect_seasonality(series: np.ndarray, periods: List[int] = [7, 30, 365]) -> float:
        """Simple seasonality detection using autocorrelation"""
        if len(series) < max(periods):
            return 0
        
        max_autocorr = 0
        for period in periods:
            if len(series) >= period * 2:
                # Calculate autocorrelation at given period
                autocorr = np.corrcoef(series[:-period], series[period:])[0, 1]
                if not np.isnan(autocorr):
                    max_autocorr = max(max_autocorr, abs(autocorr))
        
        return max_autocorr

class TrendAnalyzer:
    """
    Analysis methodologies for trend identification
    """
    
    @staticmethod
    def identify_trending_skus(sku_metrics: pd.DataFrame, trend_threshold: float = 0.05) -> Dict[str, List[str]]:
        """
        Identify trending SKUs based on various criteria
        
        Args:
            sku_metrics: DataFrame with calculated metrics
            trend_threshold: Minimum trend value to consider significant
        
        Returns:
            Dictionary categorizing trending SKUs
        """
        trending_analysis = {
            'growing_revenue': [],
            'declining_revenue': [],
            'growing_volume': [],
            'declining_volume': [],
            'high_volatility': [],
            'seasonal_patterns': [],
            'new_launches': [],
            'declining_performance': []
        }
        
        # Define thresholds
        high_volatility_threshold = sku_metrics['revenue_volatility'].quantile(0.8)
        seasonality_threshold = 0.5
        new_launch_days = 90
        
        for _, row in sku_metrics.iterrows():
            sku_id = row['sku_id']
            
            # Revenue trends
            if row.get('revenue_trend', 0) > trend_threshold:
                trending_analysis['growing_revenue'].append(sku_id)
            elif row.get('revenue_trend', 0) < -trend_threshold:
                trending_analysis['declining_revenue'].append(sku_id)
            
            # Volume trends
            if row.get('quantity_trend', 0) > trend_threshold:
                trending_analysis['growing_volume'].append(sku_id)
            elif row.get('quantity_trend', 0) < -trend_threshold:
                trending_analysis['declining_volume'].append(sku_id)
            
            # High volatility
            if row.get('revenue_volatility', 0) > high_volatility_threshold:
                trending_analysis['high_volatility'].append(sku_id)
            
            # Seasonal patterns
            if row.get('seasonality_score', 0) > seasonality_threshold:
                trending_analysis['seasonal_patterns'].append(sku_id)
            
            # New launches (less than 90 days of sales)
            if row.get('days_selling', float('inf')) < new_launch_days:
                trending_analysis['new_launches'].append(sku_id)
            
            # Declining performance (negative trends + low turnover)
            if (row.get('revenue_trend', 0) < -trend_threshold and 
                row.get('inventory_turnover', float('inf')) < 2):
                trending_analysis['declining_performance'].append(sku_id)
        
        return trending_analysis
    
    @staticmethod
    def analyze_category_performance(sales_data: pd.DataFrame, product_master: pd.DataFrame) -> pd.DataFrame:
        """
        Analyze performance by product category
        
        Args:
            sales_data: Sales transaction data
            product_master: Product master data with categories
        
        Returns:
            DataFrame with category performance metrics
        """
        # Merge sales with product data
        sales_with_category = sales_data.merge(
            product_master[['sku_id', 'category', 'subcategory', 'brand']],
            on='sku_id',
            how='left'
        )
        
        # Category-level aggregations
        category_metrics = sales_with_category.groupby(['category', 'subcategory']).agg({
            'total_revenue': ['sum', 'count'],
            'quantity_sold': 'sum',
            'unit_price': 'mean'
        }).reset_index()
        
        # Flatten columns
        category_metrics.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in category_metrics.columns.values]
        
        # Calculate growth rates if historical data available
        # This would require time-based analysis
        
        # Calculate market share within categories
        total_revenue = category_metrics['total_revenue_sum'].sum()
        category_metrics['market_share_pct'] = (category_metrics['total_revenue_sum'] / total_revenue) * 100
        
        # Sort by performance
        category_metrics = category_metrics.sort_values('total_revenue_sum', ascending=False)
        
        return category_metrics

class OpportunityScorer:
    """
    Opportunity scoring models for sales optimization
    """
    
    def __init__(self, weights: Dict[str, float] = None):
        """Initialize with scoring weights"""
        self.weights = weights or {
            'revenue_impact': 0.3,
            'margin_impact': 0.25,
            'feasibility': 0.25,
            'risk': 0.2
        }
    
    def score_opportunities(self, sku_metrics: pd.DataFrame, trending_analysis: Dict) -> pd.DataFrame:
        """
        Score opportunities for each SKU
        
        Args:
            sku_metrics: DataFrame with SKU metrics
            trending_analysis: Results from trend analysis
        
        Returns:
            DataFrame with opportunity scores and recommendations
        """
        opportunities = []
        
        for _, row in sku_metrics.iterrows():
            sku_id = row['sku_id']
            
            # Calculate component scores (0-100 scale)
            revenue_score = self._calculate_revenue_opportunity(row, trending_analysis)
            margin_score = self._calculate_margin_opportunity(row)
            feasibility_score = self._calculate_feasibility_score(row)
            risk_score = self._calculate_risk_score(row, trending_analysis)
            
            # Calculate weighted total score
            total_score = (
                revenue_score * self.weights['revenue_impact'] +
                margin_score * self.weights['margin_impact'] +
                feasibility_score * self.weights['feasibility'] +
                risk_score * self.weights['risk']
            )
            
            # Determine primary opportunity type
            opportunity_type = self._determine_opportunity_type(sku_id, trending_analysis, row)
            
            # Generate recommendations
            recommendations = self._generate_recommendations(sku_id, row, trending_analysis)
            
            opportunities.append({
                'sku_id': sku_id,
                'total_opportunity_score': total_score,
                'revenue_opportunity_score': revenue_score,
                'margin_opportunity_score': margin_score,
                'feasibility_score': feasibility_score,
                'risk_score': risk_score,
                'opportunity_type': opportunity_type,
                'recommendations': recommendations,
                'priority_level': self._assign_priority(total_score)
            })
        
        return pd.DataFrame(opportunities).sort_values('total_opportunity_score', ascending=False)
    
    def _calculate_revenue_opportunity(self, row: pd.Series, trending_analysis: Dict) -> float:
        """Calculate revenue opportunity score"""
        score = 0
        
        # High revenue, growing trend = high opportunity
        if row['abc_category'] == 'A' and row.get('revenue_trend', 0) > 0:
            score += 40
        elif row['abc_category'] == 'B' and row.get('revenue_trend', 0) > 0:
            score += 30
        
        # New launches with good initial performance
        if (row['sku_id'] in trending_analysis.get('new_launches', []) and 
            row.get('daily_revenue', 0) > row.get('avg_transaction_value', 0)):
            score += 25
        
        # Growing volume trend
        if row.get('quantity_trend', 0) > 0:
            score += 20
        
        # High transaction frequency
        if row.get('transaction_count', 0) > row.get('transaction_count', pd.Series([0])).median():
            score += 15
        
        return min(score, 100)
    
    def _calculate_margin_opportunity(self, row: pd.Series) -> float:
        """Calculate margin opportunity score"""
        score = 0
        
        # High price volatility suggests pricing optimization opportunity
        if row.get('price_volatility', 0) > 0:
            score += 30
        
        # High inventory turnover suggests pricing power
        if row.get('inventory_turnover', 0) > 6:
            score += 25
        
        # Category A items have more pricing flexibility
        if row['abc_category'] == 'A':
            score += 20
        
        # Low days of inventory suggests high demand
        if row.get('days_of_inventory', 365) < 30:
            score += 25
        
        return min(score, 100)
    
    def _calculate_feasibility_score(self, row: pd.Series) -> float:
        """Calculate implementation feasibility score"""
        score = 50  # Base score
        
        # High transaction count = easier to test changes
        if row.get('transaction_count', 0) > 100:
            score += 30
        
        # Stable performance = predictable outcomes
        if row.get('revenue_volatility', 1) < 0.5:
            score += 20
        
        # Sufficient inventory = can support increased demand
        if row.get('inventory_turnover', 0) < 8:  # Not too high turnover
            score += 10
        
        return min(score, 100)
    
    def _calculate_risk_score(self, row: pd.Series, trending_analysis: Dict) -> float:
        """Calculate risk score (higher = lower risk)"""
        score = 50  # Base score
        
        # Lower risk if growing
        if row.get('revenue_trend', 0) > 0:
            score += 25
        
        # Higher risk if declining
        if row['sku_id'] in trending_analysis.get('declining_performance', []):
            score -= 30
        
        # Higher risk if very volatile
        if row.get('revenue_volatility', 0) > 1:
            score -= 20
        
        # Lower risk if established product
        if row.get('days_selling', 0) > 365:
            score += 15
        
        # Higher risk if seasonal
        if row['sku_id'] in trending_analysis.get('seasonal_patterns', []):
            score -= 10
        
        return max(min(score, 100), 0)
    
    def _determine_opportunity_type(self, sku_id: str, trending_analysis: Dict, row: pd.Series) -> str:
        """Determine the primary opportunity type"""
        if sku_id in trending_analysis.get('growing_revenue', []):
            return 'Growth Acceleration'
        elif sku_id in trending_analysis.get('new_launches', []):
            return 'New Product Optimization'
        elif row.get('price_volatility', 0) > 0.5:
            return 'Price Optimization'
        elif row.get('inventory_turnover', 0) > 8:
            return 'Inventory Optimization'
        elif sku_id in trending_analysis.get('declining_performance', []):
            return 'Performance Recovery'
        elif row['abc_category'] == 'A':
            return 'Premium Enhancement'
        else:
            return 'General Optimization'
    
    def _generate_recommendations(self, sku_id: str, row: pd.Series, trending_analysis: Dict) -> List[str]:
        """Generate specific recommendations"""
        recommendations = []
        
        if sku_id in trending_analysis.get('growing_revenue', []):
            recommendations.append("Increase marketing investment to accelerate growth")
            recommendations.append("Consider expanding to additional channels")
        
        if row.get('inventory_turnover', 0) > 8:
            recommendations.append("Increase inventory levels to avoid stockouts")
            recommendations.append("Review reorder points and quantities")
        
        if row.get('price_volatility', 0) > 0.5:
            recommendations.append("Implement dynamic pricing strategy")
            recommendations.append("Test price points to optimize revenue")
        
        if row['abc_category'] == 'A':
            recommendations.append("Focus on customer retention programs")
            recommendations.append("Consider premium positioning or bundles")
        
        if sku_id in trending_analysis.get('declining_performance', []):
            recommendations.append("Investigate root causes of decline")
            recommendations.append("Consider promotional campaigns or repositioning")
        
        if not recommendations:
            recommendations.append("Monitor performance and market conditions")
        
        return recommendations
    
    def _assign_priority(self, score: float) -> str:
        """Assign priority level based on score"""
        if score >= 80:
            return 'Critical'
        elif score >= 60:
            return 'High'
        elif score >= 40:
            return 'Medium'
        else:
            return 'Low'

class ImplementationRoadmap:
    """
    3-month implementation roadmap framework
    """
    
    @staticmethod
    def create_roadmap(opportunities: pd.DataFrame) -> Dict[str, Dict]:
        """
        Create a 3-month implementation roadmap
        
        Args:
            opportunities: DataFrame with scored opportunities
        
        Returns:
            Dictionary with month-by-month roadmap
        """
        # Filter and prioritize opportunities
        high_priority = opportunities[opportunities['priority_level'].isin(['Critical', 'High'])]
        medium_priority = opportunities[opportunities['priority_level'] == 'Medium']
        
        roadmap = {
            'month_1': {
                'focus': 'Quick Wins and Foundation',
                'objectives': [
                    'Implement high-impact, low-effort opportunities',
                    'Establish monitoring and reporting systems',
                    'Begin data quality improvements'
                ],
                'key_initiatives': [],
                'target_skus': [],
                'expected_outcomes': [],
                'resources_needed': []
            },
            'month_2': {
                'focus': 'Medium-Term Optimizations',
                'objectives': [
                    'Execute pricing optimization strategies',
                    'Implement inventory management improvements',
                    'Launch promotional campaigns'
                ],
                'key_initiatives': [],
                'target_skus': [],
                'expected_outcomes': [],
                'resources_needed': []
            },
            'month_3': {
                'focus': 'Long-Term Strategy and Scaling',
                'objectives': [
                    'Scale successful initiatives',
                    'Implement advanced analytics',
                    'Develop predictive models'
                ],
                'key_initiatives': [],
                'target_skus': [],
                'expected_outcomes': [],
                'resources_needed': []
            }
        }
        
        # Month 1: Quick wins
        month1_opportunities = high_priority[
            high_priority['feasibility_score'] >= 70
        ].head(10)
        
        roadmap['month_1']['key_initiatives'] = [
            'Price Optimization for High-Turnover SKUs',
            'Inventory Level Adjustments',
            'Performance Dashboard Implementation',
            'Data Quality Assessment and Cleanup'
        ]
        roadmap['month_1']['target_skus'] = month1_opportunities['sku_id'].tolist()
        roadmap['month_1']['expected_outcomes'] = [
            '3-5% revenue increase from price optimization',
            '10-15% reduction in stockout events',
            'Real-time performance visibility'
        ]
        roadmap['month_1']['resources_needed'] = [
            'Data analyst (1 FTE)',
            'Business analyst (0.5 FTE)',
            'IT support for dashboard setup'
        ]
        
        # Month 2: Medium-term optimizations
        month2_opportunities = high_priority[
            ~high_priority['sku_id'].isin(month1_opportunities['sku_id'])
        ].head(15)
        
        roadmap['month_2']['key_initiatives'] = [
            'Category-Level Strategy Development',
            'Promotional Campaign Optimization',
            'Cross-Selling and Bundling Strategies',
            'Advanced Inventory Management'
        ]
        roadmap['month_2']['target_skus'] = month2_opportunities['sku_id'].tolist()
        roadmap['month_2']['expected_outcomes'] = [
            '5-8% increase in average order value',
            '20% improvement in inventory turnover',
            'Enhanced customer segmentation insights'
        ]
        roadmap['month_2']['resources_needed'] = [
            'Marketing analyst (1 FTE)',
            'Inventory manager (0.5 FTE)',
            'Additional data processing capacity'
        ]
        
        # Month 3: Long-term strategy
        month3_opportunities = pd.concat([
            medium_priority.head(20),
            high_priority[
                ~high_priority['sku_id'].isin(
                    month1_opportunities['sku_id'].tolist() + 
                    month2_opportunities['sku_id'].tolist()
                )
            ]
        ])
        
        roadmap['month_3']['key_initiatives'] = [
            'Predictive Analytics Implementation',
            'Customer Lifetime Value Optimization',
            'Market Expansion Strategies',
            'Performance Review and Strategy Refinement'
        ]
        roadmap['month_3']['target_skus'] = month3_opportunities['sku_id'].tolist()
        roadmap['month_3']['expected_outcomes'] = [
            'Predictive demand forecasting accuracy >85%',
            '10-15% improvement in customer retention',
            'Scalable framework for ongoing optimization'
        ]
        roadmap['month_3']['resources_needed'] = [
            'Data scientist (1 FTE)',
            'Business intelligence developer (0.5 FTE)',
            'Marketing automation tools'
        ]
        
        return roadmap
    
    @staticmethod
    def create_tracking_framework() -> Dict[str, any]:
        """Create framework for tracking implementation progress"""
        return {
            'kpis': {
                'financial': [
                    'Total Revenue Growth (%)',
                    'Gross Margin Improvement (%)',
                    'Revenue per SKU',
                    'Average Order Value'
                ],
                'operational': [
                    'Inventory Turnover Rate',
                    'Stockout Frequency',
                    'Order Fill Rate (%)',
                    'Days Sales Outstanding'
                ],
                'customer': [
                    'Customer Retention Rate (%)',
                    'Customer Acquisition Cost',
                    'Net Promoter Score',
                    'Purchase Frequency'
                ],
                'efficiency': [
                    'Time to Market (New Products)',
                    'Forecast Accuracy (%)',
                    'Price Change Response Time',
                    'Promotion ROI'
                ]
            },
            'reporting_frequency': {
                'daily': ['Sales Revenue', 'Inventory Levels', 'Stockout Events'],
                'weekly': ['Performance vs. Targets', 'Top/Bottom Performers', 'Inventory Turnover'],
                'monthly': ['Comprehensive Performance Review', 'ROI Analysis', 'Strategy Adjustments']
            },
            'governance': {
                'steering_committee': 'Executive oversight and strategic decisions',
                'working_group': 'Day-to-day implementation and tactical decisions',
                'data_team': 'Analytics, reporting, and technical implementation',
                'business_units': 'Implementation support and feedback'
            }
        }

class RiskAssessment:
    """
    Risk assessment and mitigation strategies framework
    """
    
    @staticmethod
    def assess_implementation_risks() -> Dict[str, Dict]:
        """
        Comprehensive risk assessment for SKU optimization implementation
        
        Returns:
            Dictionary with risk categories and mitigation strategies
        """
        return {
            'data_quality_risks': {
                'risk_level': 'High',
                'description': 'Poor data quality affecting analysis accuracy',
                'potential_impact': [
                    'Incorrect optimization decisions',
                    'Financial losses from wrong pricing',
                    'Customer dissatisfaction from stockouts'
                ],
                'probability': 'Medium-High',
                'mitigation_strategies': [
                    'Implement comprehensive data validation procedures',
                    'Establish data quality monitoring dashboards',
                    'Create data governance policies and procedures',
                    'Regular data audits and cleanup processes',
                    'Implement automated data quality checks'
                ],
                'early_warning_signs': [
                    'High percentage of missing values',
                    'Inconsistent data formats across systems',
                    'Unexplained variance in key metrics'
                ]
            },
            
            'market_volatility_risks': {
                'risk_level': 'Medium',
                'description': 'External market conditions affecting SKU performance',
                'potential_impact': [
                    'Demand fluctuations beyond forecasts',
                    'Price sensitivity changes',
                    'Competitive response to optimizations'
                ],
                'probability': 'Medium',
                'mitigation_strategies': [
                    'Implement scenario planning and stress testing',
                    'Create flexible pricing and inventory policies',
                    'Develop competitive intelligence monitoring',
                    'Build buffer stocks for high-volatility items',
                    'Establish rapid response protocols'
                ],
                'early_warning_signs': [
                    'Sudden changes in competitor pricing',
                    'Economic indicators suggesting market shifts',
                    'Unusual customer behavior patterns'
                ]
            },
            
            'technology_risks': {
                'risk_level': 'Medium',
                'description': 'Technology failures or limitations affecting implementation',
                'potential_impact': [
                    'System downtime affecting operations',
                    'Integration issues between systems',
                    'Performance degradation under load'
                ],
                'probability': 'Low-Medium',
                'mitigation_strategies': [
                    'Conduct thorough system testing before rollout',
                    'Implement backup and recovery procedures',
                    'Create manual override processes',
                    'Establish vendor support agreements',
                    'Plan phased rollout to minimize impact'
                ],
                'early_warning_signs': [
                    'System performance degradation',
                    'Integration errors or data sync issues',
                    'User complaints about system reliability'
                ]
            },
            
            'organizational_risks': {
                'risk_level': 'Medium-High',
                'description': 'Organizational resistance or capability gaps',
                'potential_impact': [
                    'Slow adoption of new processes',
                    'Inconsistent implementation across teams',
                    'Loss of institutional knowledge'
                ],
                'probability': 'Medium',
                'mitigation_strategies': [
                    'Comprehensive change management program',
                    'Extensive training and support programs',
                    'Clear communication of benefits and expectations',
                    'Incentive alignment with optimization goals',
                    'Regular feedback collection and response'
                ],
                'early_warning_signs': [
                    'Low engagement in training sessions',
                    'Resistance to new processes',
                    'High turnover in key positions'
                ]
            },
            
            'financial_risks': {
                'risk_level': 'High',
                'description': 'Financial losses from optimization mistakes',
                'potential_impact': [
                    'Revenue losses from pricing errors',
                    'Inventory write-offs from poor demand forecasting',
                    'Opportunity costs from missed optimizations'
                ],
                'probability': 'Low-Medium',
                'mitigation_strategies': [
                    'Implement gradual rollout with testing phases',
                    'Set up automated alerts for unusual performance',
                    'Create approval workflows for major changes',
                    'Maintain financial reserves for contingencies',
                    'Implement comprehensive monitoring and controls'
                ],
                'early_warning_signs': [
                    'Unusual variance from expected results',
                    'Customer complaints about pricing',
                    'Unexpected inventory accumulation or shortages'
                ]
            },
            
            'competitive_risks': {
                'risk_level': 'Medium',
                'description': 'Competitive responses negating optimization benefits',
                'potential_impact': [
                    'Price wars reducing profitability',
                    'Market share losses to competitors',
                    'Customer switching due to competitive offerings'
                ],
                'probability': 'Medium',
                'mitigation_strategies': [
                    'Develop unique value propositions beyond price',
                    'Focus on customer loyalty and retention',
                    'Create barriers to switching through superior service',
                    'Monitor competitive actions closely',
                    'Maintain pricing flexibility for rapid response'
                ],
                'early_warning_signs': [
                    'Competitor price changes following optimizations',
                    'Loss of market share in key segments',
                    'Changes in customer purchasing patterns'
                ]
            }
        }
    
    @staticmethod
    def create_risk_monitoring_dashboard() -> Dict[str, any]:
        """Create framework for ongoing risk monitoring"""
        return {
            'risk_indicators': {
                'data_quality': [
                    'Data completeness percentage',
                    'Data accuracy scores',
                    'System integration health'
                ],
                'financial': [
                    'Variance from planned revenue',
                    'Margin compression alerts',
                    'Inventory valuation changes'
                ],
                'operational': [
                    'System uptime percentage',
                    'Process compliance rates',
                    'User adoption metrics'
                ],
                'market': [
                    'Competitive pricing index',
                    'Market demand volatility',
                    'Customer satisfaction scores'
                ]
            },
            'alert_thresholds': {
                'critical': 'Immediate action required',
                'warning': 'Monitoring and preparation needed',
                'information': 'Awareness and trend tracking'
            },
            'escalation_procedures': {
                'level_1': 'Operations team response',
                'level_2': 'Management team involvement',
                'level_3': 'Executive team decision required'
            },
            'review_schedule': {
                'daily': 'Operational risk indicators',
                'weekly': 'Performance and trend analysis',
                'monthly': 'Comprehensive risk assessment',
                'quarterly': 'Strategic risk review and mitigation updates'
            }
        }

# Example Usage and Testing Functions
def generate_sample_data() -> Dict[str, pd.DataFrame]:
    """
    Generate sample data for testing the framework
    
    Returns:
        Dictionary containing sample datasets
    """
    np.random.seed(42)
    
    # Sample SKUs
    skus = [f"SKU_{i:04d}" for i in range(1, 101)]
    categories = ['Electronics', 'Clothing', 'Home & Garden', 'Sports', 'Books']
    subcategories = {
        'Electronics': ['Phones', 'Laptops', 'Accessories'],
        'Clothing': ['Shirts', 'Pants', 'Shoes'],
        'Home & Garden': ['Furniture', 'Tools', 'Decor'],
        'Sports': ['Equipment', 'Apparel', 'Accessories'],
        'Books': ['Fiction', 'Non-Fiction', 'Educational']
    }
    brands = ['Brand_A', 'Brand_B', 'Brand_C', 'Brand_D', 'Brand_E']
    
    # Product Master Data
    product_master = []
    for sku in skus:
        category = np.random.choice(categories)
        subcategory = np.random.choice(subcategories[category])
        product_master.append({
            'sku_id': sku,
            'product_name': f'Product {sku}',
            'category': category,
            'subcategory': subcategory,
            'brand': np.random.choice(brands),
            'launch_date': pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365))
        })
    product_master_df = pd.DataFrame(product_master)
    
    # Sales Data
    sales_data = []
    date_range = pd.date_range('2024-01-01', '2024-12-31', freq='D')
    
    for sku in skus:
        # Different SKUs have different performance characteristics
        base_daily_sales = np.random.exponential(scale=10)  # Heavy tail distribution
        trend = np.random.normal(0, 0.01)  # Small daily trend
        seasonality = np.sin(np.arange(len(date_range)) * 2 * np.pi / 365) * 0.2
        
        for i, date in enumerate(date_range):
            # Skip some days (not all SKUs sell every day)
            if np.random.random() > 0.3:  # 30% chance of no sales on a given day
                continue
                
            daily_factor = 1 + trend * i + seasonality[i] + np.random.normal(0, 0.1)
            daily_sales = max(1, int(base_daily_sales * daily_factor))
            
            unit_price = np.random.uniform(10, 200)
            quantity = max(1, int(np.random.exponential(2)))
            
            sales_data.append({
                'sku_id': sku,
                'transaction_date': date,
                'quantity_sold': quantity,
                'unit_price': unit_price,
                'total_revenue': quantity * unit_price,
                'discount_amount': np.random.uniform(0, quantity * unit_price * 0.1),
                'store_id': f'STORE_{np.random.randint(1, 11):02d}',
                'channel': np.random.choice(['Online', 'Retail', 'Wholesale'])
            })
    
    sales_data_df = pd.DataFrame(sales_data)
    
    # Inventory Data
    inventory_data = []
    for sku in skus:
        base_inventory = np.random.randint(50, 500)
        for date in pd.date_range('2024-01-01', '2024-12-31', freq='D'):
            # Simulate inventory fluctuations
            daily_change = np.random.randint(-20, 30)  # Daily inventory change
            on_hand = max(0, base_inventory + daily_change)
            
            inventory_data.append({
                'sku_id': sku,
                'date': date,
                'on_hand_quantity': on_hand,
                'available_quantity': max(0, on_hand - np.random.randint(0, 20)),
                'committed_quantity': np.random.randint(0, min(20, on_hand)),
                'unit_cost': np.random.uniform(5, 100)
            })
    
    inventory_data_df = pd.DataFrame(inventory_data)
    
    # Customer Data (simplified)
    customers = [f"CUST_{i:06d}" for i in range(1, 10001)]
    customer_data = []
    for customer in customers:
        customer_data.append({
            'customer_id': customer,
            'customer_segment': np.random.choice(['Premium', 'Standard', 'Budget']),
            'acquisition_date': pd.Timestamp('2020-01-01') + pd.Timedelta(days=np.random.randint(0, 1460)),
            'lifetime_value': np.random.exponential(500)
        })
    
    customer_data_df = pd.DataFrame(customer_data)
    
    # Pricing Data
    pricing_data = []
    for sku in skus:
        # Multiple price points throughout the year
        for month in range(1, 13):
            pricing_data.append({
                'sku_id': sku,
                'effective_date': pd.Timestamp(f'2024-{month:02d}-01'),
                'list_price': np.random.uniform(10, 200),
                'cost_price': np.random.uniform(5, 100),
                'competitor_price': np.random.uniform(8, 220)
            })
    
    pricing_data_df = pd.DataFrame(pricing_data)
    
    return {
        'sales_data': sales_data_df,
        'inventory_data': inventory_data_df,
        'customer_data': customer_data_df,
        'pricing_data': pricing_data_df,
        'product_master': product_master_df
    }

def run_comprehensive_analysis(data_dict: Dict[str, pd.DataFrame]) -> Dict[str, any]:
    """
    Run comprehensive SKU analysis using the framework
    
    Args:
        data_dict: Dictionary containing all required datasets
    
    Returns:
        Dictionary with complete analysis results
    """
    print("Starting Comprehensive SKU Analysis...")
    print("=" * 50)
    
    # Initialize components
    analyzer = RetailSKUAnalyzer()
    metrics_calculator = PerformanceMetricsCalculator()
    trend_analyzer = TrendAnalyzer()
    opportunity_scorer = OpportunityScorer()
    
    # Step 1: Data Quality Validation
    print("1. Validating Data Quality...")
    data_quality_results = {}
    for data_type, df in data_dict.items():
        if data_type in DataCollectionFramework.get_required_data_schema():
            validation = DataCollectionFramework.validate_data_quality(df, data_type)
            data_quality_results[data_type] = validation
            print(f"   {data_type}: {'âœ“ Valid' if validation['is_valid'] else 'âœ— Issues Found'}")
    
    # Step 2: Calculate Performance Metrics
    print("2. Calculating Performance Metrics...")
    core_metrics = metrics_calculator.calculate_core_metrics(
        data_dict['sales_data'], 
        data_dict['inventory_data']
    )
    
    advanced_metrics = metrics_calculator.calculate_advanced_metrics(
        data_dict['sales_data']
    )
    
    # Merge metrics
    sku_metrics = core_metrics.merge(advanced_metrics, on='sku_id', how='left')
    print(f"   Analyzed {len(sku_metrics)} SKUs")
    
    # Step 3: Trend Analysis
    print("3. Performing Trend Analysis...")
    trending_analysis = trend_analyzer.identify_trending_skus(sku_metrics)
    category_performance = trend_analyzer.analyze_category_performance(
        data_dict['sales_data'],
        data_dict['product_master']
    )
    
    # Step 4: Opportunity Scoring
    print("4. Scoring Opportunities...")
    opportunities = opportunity_scorer.score_opportunities(sku_metrics, trending_analysis)
    print(f"   Identified {len(opportunities)} opportunities")
    
    # Step 5: Create Implementation Roadmap
    print("5. Creating Implementation Roadmap...")
    roadmap = ImplementationRoadmap.create_roadmap(opportunities)
    tracking_framework = ImplementationRoadmap.create_tracking_framework()
    
    # Step 6: Risk Assessment
    print("6. Assessing Risks...")
    risk_assessment = RiskAssessment.assess_implementation_risks()
    risk_monitoring = RiskAssessment.create_risk_monitoring_dashboard()
    
    print("\nAnalysis Complete!")
    print("=" * 50)
    
    return {
        'data_quality': data_quality_results,
        'sku_metrics': sku_metrics,
        'trending_analysis': trending_analysis,
        'category_performance': category_performance,
        'opportunities': opportunities,
        'implementation_roadmap': roadmap,
        'tracking_framework': tracking_framework,
        'risk_assessment': risk_assessment,
        'risk_monitoring': risk_monitoring,
        'summary_stats': {
            'total_skus_analyzed': len(sku_metrics),
            'total_revenue': sku_metrics['total_revenue'].sum(),
            'avg_revenue_per_sku': sku_metrics['total_revenue'].mean(),
            'top_10_revenue_share': sku_metrics.head(10)['revenue_share_pct'].sum(),
            'critical_opportunities': len(opportunities[opportunities['priority_level'] == 'Critical']),
            'high_opportunities': len(opportunities[opportunities['priority_level'] == 'High'])
        }
    }

if __name__ == "__main__":
    """
    Example usage of the framework
    """
    print("Retail SKU Analysis Framework")
    print("=" * 50)
    print("Generating sample data for demonstration...")
    
    # Generate sample data
    sample_data = generate_sample_data()
    
    # Run comprehensive analysis
    results = run_comprehensive_analysis(sample_data)
    
    # Display key results
    print("\nKEY RESULTS SUMMARY")
    print("=" * 30)
    
    stats = results['summary_stats']
    print(f"Total SKUs Analyzed: {stats['total_skus_analyzed']:,}")
    print(f"Total Revenue: ${stats['total_revenue']:,.2f}")
    print(f"Average Revenue per SKU: ${stats['avg_revenue_per_sku']:,.2f}")
    print(f"Top 10 SKUs Revenue Share: {stats['top_10_revenue_share']:.1f}%")
    print(f"Critical Opportunities: {stats['critical_opportunities']}")
    print(f"High Priority Opportunities: {stats['high_opportunities']}")
    
    print("\nTOP 5 OPPORTUNITIES")
    print("-" * 20)
    top_opportunities = results['opportunities'].head(5)
    for _, opp in top_opportunities.iterrows():
        print(f"SKU: {opp['sku_id']}")
        print(f"  Score: {opp['total_opportunity_score']:.1f}")
        print(f"  Type: {opp['opportunity_type']}")
        print(f"  Priority: {opp['priority_level']}")
        print()
    
    print("IMPLEMENTATION ROADMAP")
    print("-" * 25)
    for month, details in results['implementation_roadmap'].items():
        print(f"\n{month.upper()}:")
        print(f"  Focus: {details['focus']}")
        print(f"  Target SKUs: {len(details['target_skus'])}")
        print(f"  Key Initiatives: {len(details['key_initiatives'])}")
    
    print("\nFramework implementation complete!")
    print("Review the generated analysis for detailed insights and recommendations.")